{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a507abc8",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) Notebook\n",
    "\n",
    "## Purpose\n",
    "This is the first and diagnostic notebook for the SkilioMall Customer Churn Prediction project.\n",
    "\n",
    "**Objectives:**\n",
    "- Explore raw data structure and characteristics\n",
    "- Visualize feature distributions and relationships\n",
    "- Identify data quality issues (missing values, outliers, imbalance)\n",
    "- Validate business assumptions about churn patterns\n",
    "- Provide recommendations for preprocessing and modeling\n",
    "\n",
    "**Output:** Comprehensive visualizations, statistical summaries, and insights for next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bb9004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c415f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# Add src\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), '..', 'src'))\n",
    "\n",
    "# Get project root\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# Import\n",
    "from data_loader import load_config\n",
    "\n",
    "# Load config\n",
    "config_path = os.path.join(project_root, 'config.yaml')\n",
    "config = load_config(config_path)\n",
    "print(f\"Configuration loaded from: {config_path}\")\n",
    "\n",
    "# --- TỰ ĐỘNG TÌM FILE DỮ LIỆU ---\n",
    "raw_rel = config['PATHS']['RAW_DATA']  # 'data/skiliomall_data.csv'\n",
    "data_path = os.path.join(project_root, raw_rel)\n",
    "\n",
    "# Nếu không tìm thấy → thử các vị trí phổ biến\n",
    "if not os.path.exists(data_path):\n",
    "    print(f\"Không tìm thấy ở: {data_path}\")\n",
    "    candidates = [\n",
    "        os.path.join(project_root, 'skiliomall_data.csv'),\n",
    "        os.path.join(project_root, 'data', 'skilio_mall_data.csv'),\n",
    "        os.path.join(os.getcwd(), 'skiliomall_data.csv'),\n",
    "        os.path.join(os.getcwd(), '..', 'skiliomall_data.csv'),\n",
    "    ]\n",
    "    for cand in candidates:\n",
    "        if os.path.exists(cand):\n",
    "            data_path = cand\n",
    "            print(f\"Tìm thấy file tại: {data_path}\")\n",
    "            break\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Không tìm thấy file skiliomall_data.csv ở bất kỳ đâu!\")\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(data_path, index_col=config['GENERAL']['ID_COLUMN'])\n",
    "print(f\"Raw data loaded: {df.shape[0]:,} rows, {df.shape[1]:,} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743f4b24",
   "metadata": {},
   "source": [
    "## Section 1: Inspect Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d337eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic dataset information\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(f\"\\nShape: {df.shape[0]:,} rows × {df.shape[1]:,} columns\")\n",
    "print(f\"\\nColumn Names and Types:\")\n",
    "print(df.dtypes)\n",
    "print(f\"\\n\\nFirst 5 Rows:\")\n",
    "print(df.head())\n",
    "print(f\"\\n\\nBasic Statistics:\")\n",
    "print(df.describe())\n",
    "print(f\"\\n\\nMissing Values:\")\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = 100 * missing / len(df)\n",
    "missing_df = pd.DataFrame({'Missing_Count': missing, 'Percentage': missing_pct})\n",
    "print(missing_df[missing_df['Missing_Count'] > 0])\n",
    "if missing_df['Missing_Count'].sum() == 0:\n",
    "    print(\"No missing values detected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7756328c",
   "metadata": {},
   "source": [
    "## Section 2: Analyze Feature Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c8bf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate numerical and categorical features\n",
    "target_col = config['GENERAL']['TARGET_COLUMN']\n",
    "\n",
    "numerical_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "if target_col in numerical_features:\n",
    "    numerical_features.remove(target_col)\n",
    "\n",
    "categorical_features = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(\"FEATURE TYPE ANALYSIS\")\n",
    "print(f\"\\nNumerical Features: {len(numerical_features)}\")\n",
    "print(numerical_features)\n",
    "print(f\"\\nCategorical Features: {len(categorical_features)}\")\n",
    "print(categorical_features)\n",
    "print(f\"\\nTarget Variable: {target_col}\")\n",
    "\n",
    "# Feature distribution summary\n",
    "print(f\"\\n\\nFeature Type Distribution:\")\n",
    "print(f\"  - Numerical: {len(numerical_features)} ({100*len(numerical_features)/(len(numerical_features)+len(categorical_features)):.1f}%)\")\n",
    "print(f\"  - Categorical: {len(categorical_features)} ({100*len(categorical_features)/(len(numerical_features)+len(categorical_features)):.1f}%)\")\n",
    "print(f\"  - Total Features: {len(numerical_features) + len(categorical_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcab505",
   "metadata": {},
   "source": [
    "## Section 3: Target Variable and Class Imbalance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0684e8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze target variable distribution\n",
    "print(\"CHURN ANALYSIS & CLASS IMBALANCE\")\n",
    "\n",
    "churn_counts = df[target_col].value_counts()\n",
    "churn_pct = df[target_col].value_counts(normalize=True) * 100\n",
    "\n",
    "print(f\"\\nChurn Distribution:\")\n",
    "print(f\"  0 (Non-Churn): {churn_counts[0]:,} ({churn_pct[0]:.2f}%)\")\n",
    "print(f\"  1 (Churn):     {churn_counts[1]:,} ({churn_pct[1]:.2f}%)\")\n",
    "\n",
    "imbalance_ratio = churn_counts[0] / churn_counts[1]\n",
    "print(f\"\\nImbalance Ratio: {imbalance_ratio:.2f}:1\")\n",
    "print(f\"This is a highly imbalanced dataset!\")\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "axes[0].bar(['Non-Churn (0)', 'Churn (1)'], churn_counts.values, color=colors, alpha=0.8, edgecolor='black')\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_title('Churn Distribution (Count)', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (v, p) in enumerate(zip(churn_counts.values, churn_pct.values)):\n",
    "    axes[0].text(i, v + max(churn_counts.values)*0.02, f'{v:,}\\n({p:.1f}%)', \n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(churn_counts.values, labels=['Non-Churn (0)', 'Churn (1)'], autopct='%1.1f%%',\n",
    "           colors=colors, startangle=90, textprops={'fontsize': 11, 'fontweight': 'bold'})\n",
    "axes[1].set_title('Churn Distribution (Percentage)', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nKey Insight:\")\n",
    "print(f\"   - Only {churn_pct[1]:.2f}% of customers have churned\")\n",
    "print(f\"   - This is a binary classification problem with significant class imbalance\")\n",
    "print(f\"   - Consider using techniques like SMOTE, class weights, or PR-AUC for modeling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b790753e",
   "metadata": {},
   "source": [
    "## Section 4: Distribution of Key Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5418292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select key features for detailed distribution analysis\n",
    "key_features = ['age', 'aov_2024', 'rfm_recency', 'sessions_30d', 'orders_2024']\n",
    "\n",
    "# Filter to available features in dataset\n",
    "key_features = [f for f in key_features if f in numerical_features]\n",
    "\n",
    "print(\"KEY NUMERICAL FEATURE DISTRIBUTIONS\")\n",
    "print(f\"\\nAnalyzing {len(key_features)} features: {key_features}\\n\")\n",
    "\n",
    "# Create comprehensive distribution visualization\n",
    "fig, axes = plt.subplots(len(key_features), 1, figsize=(14, 4*len(key_features)))\n",
    "if len(key_features) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, feature in enumerate(key_features):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Histogram with KDE\n",
    "    data = df[feature].dropna()\n",
    "    ax.hist(data, bins=50, density=True, alpha=0.6, color='skyblue', edgecolor='black', label='Histogram')\n",
    "    \n",
    "    # Add KDE line\n",
    "    from scipy.stats import gaussian_kde\n",
    "    kde = gaussian_kde(data)\n",
    "    x_range = np.linspace(data.min(), data.max(), 200)\n",
    "    ax.plot(x_range, kde(x_range), 'r-', linewidth=2, label='KDE')\n",
    "    \n",
    "    # Styling\n",
    "    ax.set_xlabel(feature, fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Density', fontsize=11)\n",
    "    ax.set_title(f'Distribution of {feature.upper()}', fontsize=12, fontweight='bold')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    # Add statistics text\n",
    "    skewness = stats.skew(data)\n",
    "    kurtosis_val = stats.kurtosis(data)\n",
    "    textstr = f'Mean: {data.mean():.2f}\\nStd: {data.std():.2f}\\nSkew: {skewness:.2f}\\nKurt: {kurtosis_val:.2f}'\n",
    "    ax.text(0.02, 0.98, textstr, transform=ax.transAxes, fontsize=9,\n",
    "           verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nSummary Statistics for Key Features:\")\n",
    "for feature in key_features:\n",
    "    data = df[feature].dropna()\n",
    "    print(f\"\\n{feature.upper()}:\")\n",
    "    print(f\"  Count:      {len(data):,}\")\n",
    "    print(f\"  Mean:       {data.mean():.2f}\")\n",
    "    print(f\"  Median:     {data.median():.2f}\")\n",
    "    print(f\"  Std Dev:    {data.std():.2f}\")\n",
    "    print(f\"  Min:        {data.min():.2f}\")\n",
    "    print(f\"  Max:        {data.max():.2f}\")\n",
    "    print(f\"  Skewness:   {stats.skew(data):.3f} {'(Right-skewed)' if stats.skew(data) > 0.5 else '(Left-skewed)' if stats.skew(data) < -0.5 else '(Symmetric)'}\")\n",
    "    print(f\"  Kurtosis:   {stats.kurtosis(data):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daea4ccd",
   "metadata": {},
   "source": [
    "## Section 5: Outlier Detection and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64efd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform comprehensive outlier detection using IQR method\n",
    "print(\"OUTLIER DETECTION & ANALYSIS\")\n",
    "\n",
    "outlier_summary = []\n",
    "\n",
    "for feature in numerical_features:\n",
    "    data = df[feature].dropna()\n",
    "    \n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers_mask = (data < lower_bound) | (data > upper_bound)\n",
    "    n_outliers = outliers_mask.sum()\n",
    "    outlier_pct = 100 * n_outliers / len(data)\n",
    "    \n",
    "    outlier_summary.append({\n",
    "        'Feature': feature,\n",
    "        'Outlier_Count': n_outliers,\n",
    "        'Outlier_%': outlier_pct,\n",
    "        'Skewness': stats.skew(data),\n",
    "        'Kurtosis': stats.kurtosis(data),\n",
    "        'Lower_Bound': lower_bound,\n",
    "        'Upper_Bound': upper_bound\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame and sort by outlier percentage\n",
    "outlier_df = pd.DataFrame(outlier_summary).sort_values('Outlier_%', ascending=False)\n",
    "\n",
    "print(\"\\nOutlier Summary (IQR Method):\")\n",
    "print(outlier_df.to_string(index=False))\n",
    "\n",
    "# Categorize features\n",
    "high_outlier_features = outlier_df[outlier_df['Outlier_%'] > 5]['Feature'].tolist()\n",
    "moderate_outlier_features = outlier_df[(outlier_df['Outlier_%'] > 2) & (outlier_df['Outlier_%'] <= 5)]['Feature'].tolist()\n",
    "clean_features = outlier_df[outlier_df['Outlier_%'] <= 2]['Feature'].tolist()\n",
    "\n",
    "print(f\"\\n\\nFeature Categories:\")\n",
    "print(f\"  High Outliers (>5%): {high_outlier_features}\")\n",
    "print(f\"  Moderate Outliers (2-5%): {moderate_outlier_features}\")\n",
    "print(f\"  Clean Features (<=2%): {clean_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e592f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boxplots and Q-Q plots for all numerical features\n",
    "n_features = len(numerical_features)\n",
    "n_rows = (n_features + 1) // 2\n",
    "\n",
    "# Boxplots for outlier visualization\n",
    "fig, axes = plt.subplots(n_rows, 2, figsize=(15, 4*n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(numerical_features):\n",
    "    ax = axes[idx]\n",
    "    data = df[feature].dropna()\n",
    "    \n",
    "    bp = ax.boxplot([data], labels=[feature], patch_artist=True, vert=True,\n",
    "                     boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
    "                     medianprops=dict(color='red', linewidth=2),\n",
    "                     whiskerprops=dict(linewidth=1.5),\n",
    "                     capprops=dict(linewidth=1.5))\n",
    "    \n",
    "    # Add outlier information\n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers_mask = (data < Q1 - 1.5*IQR) | (data > Q3 + 1.5*IQR)\n",
    "    n_outliers = outliers_mask.sum()\n",
    "    outlier_pct = 100 * n_outliers / len(data)\n",
    "    \n",
    "    ax.set_ylabel('Value', fontsize=10)\n",
    "    ax.set_title(f'{feature}\\n({n_outliers} outliers, {outlier_pct:.1f}%)', \n",
    "                fontweight='bold', fontsize=11)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Remove extra subplots\n",
    "for idx in range(len(numerical_features), len(axes)):\n",
    "    fig.delaxes(axes[idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBoxplots generated - visual inspection of outliers complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e226b9",
   "metadata": {},
   "source": [
    "## Section 6: Feature-Churn Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2159ff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze relationships between features and churn\n",
    "print(\"FEATURE-CHURN RELATIONSHIPS\")\n",
    "\n",
    "# Calculate correlation with churn\n",
    "correlations = df[numerical_features + [target_col]].corr()[target_col].drop(target_col).sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nCorrelation with Churn (Pearson):\")\n",
    "print(correlations)\n",
    "\n",
    "print(\"\\n\\nFeatures with Strongest Correlation to Churn:\")\n",
    "top_pos = correlations[correlations > 0].head(5)\n",
    "top_neg = correlations[correlations < 0].tail(5)\n",
    "print(\"\\nPositive Correlation (features increase -> churn increases):\")\n",
    "for feat, corr in top_pos.items():\n",
    "    print(f\"  {feat:20s}: {corr:+.4f}\")\n",
    "print(\"\\nNegative Correlation (features increase -> churn decreases):\")\n",
    "for feat, corr in top_neg.items():\n",
    "    print(f\"  {feat:20s}: {corr:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5c2dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualizations for key features by churn status\n",
    "features_to_compare = correlations.abs().nlargest(5).index.tolist()\n",
    "\n",
    "fig, axes = plt.subplots(len(features_to_compare), 1, figsize=(14, 5*len(features_to_compare)))\n",
    "if len(features_to_compare) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, feature in enumerate(features_to_compare):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Prepare data\n",
    "    churn_0 = df[df[target_col] == 0][feature].dropna()\n",
    "    churn_1 = df[df[target_col] == 1][feature].dropna()\n",
    "    \n",
    "    # Create violin plot\n",
    "    parts = ax.violinplot([churn_0, churn_1], positions=[0, 1], showmeans=True, showmedians=True)\n",
    "    \n",
    "    # Add boxplots on top\n",
    "    bp = ax.boxplot([churn_0, churn_1], positions=[0, 1], widths=0.1, \n",
    "                     patch_artist=True, showfliers=False,\n",
    "                     boxprops=dict(facecolor='yellow', alpha=0.5),\n",
    "                     medianprops=dict(color='red', linewidth=2))\n",
    "    \n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_xticklabels(['Non-Churn (0)', 'Churn (1)'])\n",
    "    ax.set_ylabel(feature, fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'{feature} Distribution by Churn Status\\n(Correlation: {correlations[feature]:+.4f})', \n",
    "                fontsize=12, fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add statistics\n",
    "    stats_text = (f\"Non-Churn: μ={churn_0.mean():.2f}, σ={churn_0.std():.2f}\\n\"\n",
    "                 f\"Churn:     μ={churn_1.mean():.2f}, σ={churn_1.std():.2f}\")\n",
    "    ax.text(0.98, 0.97, stats_text, transform=ax.transAxes, fontsize=9,\n",
    "           verticalalignment='top', horizontalalignment='right',\n",
    "           bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFeature-churn comparison plots generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c6a8f1",
   "metadata": {},
   "source": [
    "## Key Findings and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80030584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary report\n",
    "print(\"EDA SUMMARY REPORT\")\n",
    "\n",
    "summary_report = f\"\"\"\n",
    "DATASET OVERVIEW\n",
    "- Total Records: {df.shape[0]:,}\n",
    "- Total Features: {df.shape[1]:,} ({len(numerical_features)} numerical, {len(categorical_features)} categorical)\n",
    "- Missing Values: {df.isnull().sum().sum()} (No data quality issues)\n",
    "\n",
    "TARGET VARIABLE (CHURN)\n",
    "- Non-Churn (0): {churn_counts[0]:,} ({churn_pct[0]:.2f}%)\n",
    "- Churn (1):     {churn_counts[1]:,} ({churn_pct[1]:.2f}%)\n",
    "- Class Imbalance Ratio: {imbalance_ratio:.2f}:1\n",
    "- Highly imbalanced dataset - requires special handling in modeling phase\n",
    "\n",
    "KEY DISTRIBUTIONS\n",
    "- Key Features Analyzed: {', '.join(key_features)}\n",
    "- Normally Distributed: {len(clean_features)} features\n",
    "- Skewed Features: {len(high_outlier_features) + len(moderate_outlier_features)} features need transformation\n",
    "- High Outlier Features (>5%): {', '.join(high_outlier_features) if high_outlier_features else 'None'}\n",
    "\n",
    "OUTLIER ANALYSIS\n",
    "- Features with High Outlier %: {len(high_outlier_features)} features\n",
    "- Features with Moderate Outliers: {len(moderate_outlier_features)} features\n",
    "- Clean Features (<=2% outliers): {len(clean_features)} features\n",
    "\n",
    "FEATURE-CHURN RELATIONSHIPS\n",
    "- Top Positive Correlation: {correlations.idxmax()} ({correlations.max():.4f})\n",
    "  Higher values associated with INCREASED churn risk\n",
    "- Top Negative Correlation: {correlations.idxmin()} ({correlations.min():.4f})\n",
    "  Higher values associated with DECREASED churn risk\n",
    "\n",
    "RECOMMENDATIONS FOR NEXT STEPS\n",
    "\n",
    "1. DATA PREPROCESSING:\n",
    "   - Apply log transformation to highly skewed features\n",
    "   - Handle zero-inflated features with binary indicators\n",
    "   - Standardize/normalize features for tree-based vs linear models\n",
    "   - Consider outlier removal or capping for extreme values\n",
    "\n",
    "2. FEATURE ENGINEERING:\n",
    "   - Create interaction features between high-correlation pairs\n",
    "   - Apply dimensionality reduction if needed (PCA)\n",
    "   - Encode categorical variables appropriately\n",
    "\n",
    "3. MODELING CONSIDERATIONS:\n",
    "   - Use stratified train-test split to maintain class distribution\n",
    "   - Apply class weights or SMOTE for imbalanced data handling\n",
    "   - Use PR-AUC instead of ROC-AUC for evaluation (more suitable for imbalanced data)\n",
    "   - Consider ensemble methods (Random Forest, XGBoost, LightGBM)\n",
    "   - Use Lift/Gain metrics for business interpretation\n",
    "\n",
    "4. BASELINE MODELS TO TRY:\n",
    "   - Logistic Regression (baseline)\n",
    "   - Random Forest (handles non-linearity)\n",
    "   - XGBoost with class weights\n",
    "   - LightGBM with appropriate parameters\n",
    "\"\"\"\n",
    "\n",
    "print(summary_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b22575",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "**Data Quality Assessment**: No missing values, clean dataset ready for processing\n",
    "\n",
    "**Feature Landscape**: 27 numerical features with diverse distributions - some skewed, some with outliers\n",
    "\n",
    "**Imbalance Confirmation**: ~25% churn rate represents significant class imbalance, requiring specialized modeling techniques\n",
    "\n",
    "**Feature-Churn Insights**: Identified key features with strong correlations to churn for prioritization in modeling\n",
    "\n",
    "**Data Quality Issues**: Identified outliers and skewed distributions requiring preprocessing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
