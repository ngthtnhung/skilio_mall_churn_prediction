{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ff488f3",
   "metadata": {},
   "source": [
    "# SkilioMall Churn Prediction - Training Pipeline\n",
    "\n",
    "This notebook implements a complete machine learning pipeline for customer churn prediction. The workflow includes data loading, preprocessing, feature engineering, model training, evaluation, and interpretability analysis.\n",
    "\n",
    "Pipeline Overview:\n",
    "1. Configuration and module setup\n",
    "2. Data loading and splitting\n",
    "3. Feature engineering\n",
    "4. Preprocessing and transformation\n",
    "5. Baseline model training\n",
    "6. Hyperparameter tuning\n",
    "7. Threshold optimization\n",
    "8. Final model training\n",
    "9. Evaluation on test set\n",
    "10. Model visualization and interpretation\n",
    "11. Artifact storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffb303d",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "Load configuration file, set up project paths, and import required libraries. This cell also establishes fallback mechanisms for custom modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee747c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (roc_auc_score, average_precision_score,\n",
    "                             precision_recall_curve, precision_score,\n",
    "                             recall_score, f1_score, confusion_matrix)\n",
    "import shap\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Set up project root directory and add src to path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.insert(0, os.path.join(project_root, 'src'))\n",
    "\n",
    "# Load configuration from YAML file\n",
    "config_path = os.path.join(project_root, 'config.yaml')\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Extract key configuration parameters\n",
    "RANDOM_STATE = config['GENERAL']['RANDOM_STATE']\n",
    "TARGET_COL = config['GENERAL']['TARGET_COLUMN']\n",
    "ID_COL = config['GENERAL'].get('ID_COLUMN', None)  # ID_COL can be None\n",
    "OUTPUTS_DIR = os.path.join(project_root, config['PATHS']['OUTPUTS_DIR'])\n",
    "\n",
    "# Create outputs directory if it doesn't exist\n",
    "os.makedirs(OUTPUTS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Random state: {RANDOM_STATE}\")\n",
    "print(f\"Target column: {TARGET_COL}\")\n",
    "print(f\"Outputs directory: outputs/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35cb694",
   "metadata": {},
   "source": [
    "## 2. Module Imports with Fallback Mechanisms\n",
    "\n",
    "Import custom modules from the src directory. If modules are not available, implement fallback functions to ensure the notebook can still run independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a40426",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from data_loader import load_config, load_raw_data\n",
    "except Exception as e:\n",
    "    print(\"Warning: data_loader.load_config or load_raw_data not found. Using simple loader fallback.\")\n",
    "    def load_raw_data(config_path):\n",
    "        # fallback: read path from config yaml we already loaded\n",
    "        raw_rel = config['PATHS']['RAW_DATA']\n",
    "        data_path = os.path.join(project_root, raw_rel)\n",
    "        if not os.path.exists(data_path):\n",
    "            raise FileNotFoundError(f\"Raw data not found: {data_path}\")\n",
    "        df = pd.read_csv(data_path, index_col=ID_COL)\n",
    "        return df\n",
    "\n",
    "try:\n",
    "    from features import create_all_features as create_features\n",
    "except Exception as e:\n",
    "    print(\"Warning: features.create_all_features not found. Using identity fallback.\")\n",
    "    def create_features(df):\n",
    "        # fallback: return copy (no new features)\n",
    "        return df.copy()\n",
    "\n",
    "try:\n",
    "    from preprocessing import build_preprocessor\n",
    "except Exception as e:\n",
    "    print(\"Warning: preprocessing.build_preprocessor not found. Using simple numeric/categorical pipeline fallback.\")\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "    from sklearn.impute import SimpleImputer\n",
    "\n",
    "    def build_preprocessor(numerical_features=None, categorical_features=None):\n",
    "        # if None, determine later from training set\n",
    "        num_pipe = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        cat_pipe = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "        ])\n",
    "        # we return a factory that we'll call with feature lists after FE\n",
    "        def factory(num_feats, cat_feats):\n",
    "            return ColumnTransformer([\n",
    "                ('num', num_pipe, num_feats),\n",
    "                ('cat', cat_pipe, cat_feats),\n",
    "            ], sparse_threshold=0)\n",
    "        return factory\n",
    "\n",
    "try:\n",
    "    from modeling import train_baseline_models, tune_model_optuna, fit_final_model\n",
    "except Exception as e:\n",
    "    print(\"Warning: modeling module functions missing. Using simple implementations.\")\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    import lightgbm as lgb\n",
    "    import xgboost as xgb\n",
    "\n",
    "    def train_baseline_models(X_train, y_train, X_val, y_val, random_state=RANDOM_STATE):\n",
    "        results = {}\n",
    "\n",
    "        # Logistic Regression\n",
    "        lr = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=random_state)\n",
    "        lr.fit(X_train, y_train)\n",
    "        preds = lr.predict_proba(X_val)[:,1]\n",
    "        results['LogisticRegression'] = {'model': lr, 'val_proba': preds}\n",
    "\n",
    "        # Random Forest\n",
    "        rf = RandomForestClassifier(n_estimators=200, class_weight='balanced', random_state=random_state)\n",
    "        rf.fit(X_train, y_train)\n",
    "        preds = rf.predict_proba(X_val)[:,1]\n",
    "        results['RandomForest'] = {'model': rf, 'val_proba': preds}\n",
    "\n",
    "        # XGBoost\n",
    "        scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "        xgb_model = xgb.XGBClassifier(\n",
    "            n_estimators=500,\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "            random_state=random_state,\n",
    "            eval_metric='logloss'\n",
    "        )\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "        preds = xgb_model.predict_proba(X_val)[:,1]\n",
    "        results['XGBoost'] = {'model': xgb_model, 'val_proba': preds}\n",
    "\n",
    "        # LightGBM\n",
    "        lgbm = lgb.LGBMClassifier(n_estimators=500, class_weight='balanced', random_state=random_state)\n",
    "        lgbm.fit(X_train, y_train)\n",
    "        preds = lgbm.predict_proba(X_val)[:,1]\n",
    "        results['LightGBM'] = {'model': lgbm, 'val_proba': preds}\n",
    "\n",
    "        return results\n",
    "\n",
    "    def tune_model_optuna(X_train, y_train, X_val, y_val, time_budget_s=600, random_state=RANDOM_STATE):\n",
    "        # Simple placeholder: return LightGBM trained on train+val without real tuning\n",
    "        import lightgbm as lgb\n",
    "        model = lgb.LGBMClassifier(n_estimators=1000, learning_rate=0.03, class_weight='balanced', random_state=random_state)\n",
    "        model.fit(X_train, y_train)\n",
    "        return model, {'n_estimators':1000, 'learning_rate':0.03}\n",
    "\n",
    "    def fit_final_model(X_train, y_train, params=None, random_state=RANDOM_STATE):\n",
    "        import lightgbm as lgb\n",
    "        params = params or {}\n",
    "        model = lgb.LGBMClassifier(random_state=random_state, **params)\n",
    "        model.fit(X_train, y_train)\n",
    "        return model\n",
    "\n",
    "try:\n",
    "    from evaluation import (compute_threshold_metrics, plot_lift_gain, plot_calibration_curve,\n",
    "                            plot_confusion_matrix, compute_lift_at_k)\n",
    "except Exception as e:\n",
    "    print(\"Warning: evaluation helpers missing. Using simple local implementations.\")\n",
    "    def compute_threshold_metrics(y_true, y_proba, threshold=0.5):\n",
    "        y_pred = (y_proba >= threshold).astype(int)\n",
    "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        return {'precision': precision, 'recall': recall, 'f1': f1, 'confusion_matrix': cm}\n",
    "\n",
    "    def compute_lift_at_k(y_true, y_proba, k=0.2):\n",
    "        # returns recall@topk and lift@topk\n",
    "        df_tmp = pd.DataFrame({'y': y_true, 'p': y_proba})\n",
    "        df_tmp = df_tmp.sort_values('p', ascending=False).reset_index(drop=True)\n",
    "        top_n = int(len(df_tmp) * k)\n",
    "        top = df_tmp.iloc[:top_n]\n",
    "        recall_at_k = top['y'].sum() / df_tmp['y'].sum() if df_tmp['y'].sum() > 0 else 0\n",
    "        lift = (top['y'].mean() / df_tmp['y'].mean()) if df_tmp['y'].mean() > 0 else np.nan\n",
    "        return {'recall_at_k': recall_at_k, 'lift_at_k': lift}\n",
    "\n",
    "    def plot_lift_gain(y_true, y_proba, k=0.2):\n",
    "        out = compute_lift_at_k(y_true, y_proba, k)\n",
    "        print(f\"Recall@Top{k*100:.0f}% = {out['recall_at_k']:.3f}; Lift@Top{k*100:.0f}% = {out['lift_at_k']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8425df28",
   "metadata": {},
   "source": [
    "## 3. Load Raw Data\n",
    "\n",
    "Load the raw dataset from CSV file. Perform basic validation to ensure the target column exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f42ad87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data path from configuration\n",
    "raw_rel = config['PATHS']['RAW_DATA']\n",
    "data_path = os.path.join(project_root, raw_rel)\n",
    "\n",
    "# Validate file existence\n",
    "if not os.path.exists(data_path):\n",
    "    raise FileNotFoundError(f\"Raw data file not found: {data_path}\")\n",
    "\n",
    "# Load data using custom loader if available, otherwise use pandas\n",
    "df = load_raw_data(config_path) if 'load_raw_data' in globals() else pd.read_csv(data_path, index_col=ID_COL)\n",
    "\n",
    "print(\"Raw data shape:\", df.shape)\n",
    "\n",
    "# Validate that target column exists in the dataset\n",
    "if TARGET_COL not in df.columns:\n",
    "    raise KeyError(f\"Target column '{TARGET_COL}' not found in data. Columns: {df.columns.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed5d39b",
   "metadata": {},
   "source": [
    "## 4. Train/Validation/Test Split\n",
    "\n",
    "Split the dataset into three sets using stratified sampling to maintain class distribution:\n",
    "- Training set: for model training\n",
    "- Validation set: for hyperparameter tuning and threshold selection\n",
    "- Test set: for final unbiased evaluation\n",
    "\n",
    "The split is performed in two steps to ensure correct proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac6511e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get split ratios from configuration\n",
    "train_ratio = config['DATA_SPLIT']['TRAIN_RATIO']\n",
    "val_ratio = config['DATA_SPLIT']['VAL_RATIO']\n",
    "test_ratio = config['DATA_SPLIT']['TEST_RATIO']\n",
    "\n",
    "# Ensure ratios sum to 1.0\n",
    "assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \"Train/Val/Test ratios must sum to 1\"\n",
    "\n",
    "# Step 1: Split into training and temporary set (validation + test)\n",
    "temp_size = val_ratio + test_ratio\n",
    "train_df, temp_df = train_test_split(df, test_size=temp_size, stratify=df[TARGET_COL], random_state=RANDOM_STATE)\n",
    "\n",
    "# Step 2: Split temporary set into validation and test sets proportionally\n",
    "val_df, test_df = train_test_split(temp_df, test_size=(test_ratio/temp_size), stratify=temp_df[TARGET_COL], random_state=RANDOM_STATE)\n",
    "\n",
    "# Display split sizes and class distributions\n",
    "print(\"Split sizes:\", train_df.shape, val_df.shape, test_df.shape)\n",
    "print(\"Train/Val/Test churn distribution:\")\n",
    "print(train_df[TARGET_COL].value_counts(normalize=True))\n",
    "print(val_df[TARGET_COL].value_counts(normalize=True))\n",
    "print(test_df[TARGET_COL].value_counts(normalize=True))\n",
    "\n",
    "# Save checkpoint of splits for reproducibility\n",
    "checkpoint_path = os.path.join(project_root, config['PATHS']['CHECKPOINT_DATA'])\n",
    "os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "pd.to_pickle({'train': train_df, 'val': val_df, 'test': test_df}, checkpoint_path)\n",
    "print(\"Saved data split checkpoint:\", config['PATHS']['CHECKPOINT_DATA'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b0794b",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering\n",
    "\n",
    "Apply feature engineering transformations to create new predictive features. The same transformations are applied consistently to train, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046d884c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Applying feature engineering (create_features)...\")\n",
    "# If create_features expects (X, y) or DataFrame, we try both\n",
    "def apply_create_features(df_in):\n",
    "    try:\n",
    "        out = create_features(df_in)\n",
    "        return out\n",
    "    except Exception:\n",
    "        try:\n",
    "            # if signature create_features(X, y)\n",
    "            X = df_in.drop(columns=[TARGET_COL])\n",
    "            y = df_in[TARGET_COL]\n",
    "            X_new = create_features(X, y)\n",
    "            # attach target back\n",
    "            X_new[TARGET_COL] = y.values\n",
    "            return X_new\n",
    "        except Exception as e:\n",
    "            print(\"Feature function failed:\", e)\n",
    "            return df_in.copy()\n",
    "\n",
    "train_fe = apply_create_features(train_df)\n",
    "val_fe = apply_create_features(val_df)\n",
    "test_fe = apply_create_features(test_df)\n",
    "\n",
    "print(\"After FE shapes:\", train_fe.shape, val_fe.shape, test_fe.shape)\n",
    "\n",
    "# Separate X/y after FE\n",
    "X_train = train_fe.drop(columns=[TARGET_COL])\n",
    "y_train = train_fe[TARGET_COL].astype(int)\n",
    "\n",
    "X_val = val_fe.drop(columns=[TARGET_COL])\n",
    "y_val = val_fe[TARGET_COL].astype(int)\n",
    "\n",
    "X_test = test_fe.drop(columns=[TARGET_COL])\n",
    "y_test = test_fe[TARGET_COL].astype(int)\n",
    "\n",
    "# Identify numerical/categorical features from config\n",
    "num_feats = config['FEATURE_ENGINEERING']['NUMERICAL_FEATURES']\n",
    "cat_feats = config['FEATURE_ENGINEERING']['CATEGORICAL_FEATURES']\n",
    "\n",
    "print(f\"Numerical features: {len(num_feats)}; Categorical features: {len(cat_feats)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c560cea1",
   "metadata": {},
   "source": [
    "## 6. Data Preprocessing\n",
    "\n",
    "Build and fit preprocessing pipeline on training data only to avoid data leakage. The pipeline handles:\n",
    "- Numerical features: imputation and standardization\n",
    "- Categorical features: imputation and one-hot encoding\n",
    "\n",
    "The fitted preprocessor is then applied to validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117b2ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building preprocessor and fitting on train only...\")\n",
    "preprocessor_factory = build_preprocessor()\n",
    "# If build_preprocessor returned a factory (our fallback), call it; otherwise it's a fitted transformer\n",
    "if callable(preprocessor_factory):\n",
    "    preprocessor = preprocessor_factory(num_feats, cat_feats)\n",
    "else:\n",
    "    preprocessor = preprocessor_factory  # assume it's a transformer class\n",
    "\n",
    "# Fit preprocessor on X_train only\n",
    "preprocessor = preprocessor.fit(X_train)\n",
    "X_train_prep = preprocessor.transform(X_train)\n",
    "X_val_prep = preprocessor.transform(X_val)\n",
    "X_test_prep = preprocessor.transform(X_test)\n",
    "\n",
    "# If transformer returns numpy arrays, convert to DataFrame for SHAP and interpretability (keep column names)\n",
    "def get_feature_names_from_preprocessor(preproc, num_feats, cat_feats):\n",
    "    # best-effort extraction\n",
    "    try:\n",
    "        # If it's ColumnTransformer with named transformers\n",
    "        if hasattr(preproc, 'transformers_'):\n",
    "            feature_names = []\n",
    "            for name, trans, cols in preproc.transformers_:\n",
    "                if name == 'num':\n",
    "                    feature_names.extend(cols)\n",
    "                elif name == 'cat':\n",
    "                    # try to get onehot feature names\n",
    "                    ohe = trans.named_steps.get('ohe') or trans.named_steps.get('onehot')\n",
    "                    if ohe is not None:\n",
    "                        names = ohe.get_feature_names_out(cols)\n",
    "                        feature_names.extend(list(names))\n",
    "                    else:\n",
    "                        feature_names.extend(cols)\n",
    "            return feature_names\n",
    "    except Exception:\n",
    "        pass\n",
    "    # fallback\n",
    "    return list(num_feats) + list(cat_feats)\n",
    "\n",
    "feature_names = get_feature_names_from_preprocessor(preprocessor, num_feats, cat_feats)\n",
    "print(\"Prepared feature names length:\", len(feature_names))\n",
    "\n",
    "# Convert prepared arrays to DataFrames if necessary\n",
    "if isinstance(X_train_prep, np.ndarray):\n",
    "    X_train_prep_df = pd.DataFrame(X_train_prep, columns=feature_names, index=X_train.index)\n",
    "    X_val_prep_df = pd.DataFrame(X_val_prep, columns=feature_names, index=X_val.index)\n",
    "    X_test_prep_df = pd.DataFrame(X_test_prep, columns=feature_names, index=X_test.index)\n",
    "else:\n",
    "    X_train_prep_df = X_train_prep\n",
    "    X_val_prep_df = X_val_prep\n",
    "    X_test_prep_df = X_test_prep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaeea2bd",
   "metadata": {},
   "source": [
    "## 7. Baseline Model Training\n",
    "\n",
    "Train multiple baseline models to compare performance:\n",
    "- Logistic Regression\n",
    "- Random Forest\n",
    "- XGBoost\n",
    "- LightGBM\n",
    "\n",
    "Evaluate each model on the validation set using ROC-AUC and PR-AUC metrics. Select the best performing model for further tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5468eb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training baseline models (Logistic, RandomForest, XGBoost, LightGBM)...\")\n",
    "baseline_results = train_baseline_models(X_train_prep_df.values, y_train.values, X_val_prep_df.values, y_val.values, random_state=RANDOM_STATE)\n",
    "# baseline_results is expected dict: name -> {'model': model, 'val_proba': preds}\n",
    "print(\"Baseline models trained:\", list(baseline_results.keys()))\n",
    "\n",
    "# Evaluate baselines on validation\n",
    "def evaluate_probas(y_true, y_proba):\n",
    "    roc = roc_auc_score(y_true, y_proba)\n",
    "    pr = average_precision_score(y_true, y_proba)\n",
    "    return {'roc_auc': roc, 'pr_auc': pr}\n",
    "\n",
    "baseline_metrics = {}\n",
    "for name, info in baseline_results.items():\n",
    "    proba = info.get('val_proba') if isinstance(info, dict) else None\n",
    "    model = info.get('model') if isinstance(info, dict) else info\n",
    "    if proba is None:\n",
    "        proba = model.predict_proba(X_val_prep_df.values)[:,1]\n",
    "    baseline_metrics[name] = evaluate_probas(y_val.values, proba)\n",
    "print(\"Baseline validation metrics (ROC-AUC, PR-AUC):\")\n",
    "for k,v in baseline_metrics.items():\n",
    "    print(k, v)\n",
    "\n",
    "# Choose best model by ROC-AUC or PR-AUC\n",
    "best_name = max(baseline_metrics.items(), key=lambda x: (x[1]['roc_auc'], x[1]['pr_auc']))[0]\n",
    "print(\"Selected best baseline:\", best_name)\n",
    "\n",
    "best_model = baseline_results[best_name]['model']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c43a7f",
   "metadata": {},
   "source": [
    "## 8. Hyperparameter Tuning\n",
    "\n",
    "Perform hyperparameter optimization using Optuna or fallback to default parameters. This step aims to improve model performance beyond the baseline configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db7e3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_tune = True\n",
    "if do_tune:\n",
    "    print(\"Tuning best model lightly (Optuna or fallback)...\")\n",
    "    try:\n",
    "        tuned_model, best_params = tune_model_optuna(X_train_prep_df.values, y_train.values, X_val_prep_df.values, y_val.values, time_budget_s=900, random_state=RANDOM_STATE)\n",
    "        final_model = tuned_model\n",
    "        print(\"Tuning complete. Best params:\", best_params)\n",
    "    except Exception as e:\n",
    "        print(\"Tuning failed or not implemented in modeling. Using selected baseline model.\")\n",
    "        final_model = best_model\n",
    "else:\n",
    "    final_model = best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405f6155",
   "metadata": {},
   "source": [
    "## 9. Optimal Threshold Selection\n",
    "\n",
    "Select the optimal classification threshold on the validation set by maximizing the F1 score. This threshold will be used for final predictions on the test set.\n",
    "\n",
    "Note: This is done BEFORE retraining to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ba9116",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Selecting optimal threshold on validation set...\")\n",
    "val_proba_before_retrain = final_model.predict_proba(X_val_prep_df.values)[:,1]\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_val.values, val_proba_before_retrain)\n",
    "f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-12)\n",
    "best_idx = np.nanargmax(f1_scores)\n",
    "# Fix: ensure best_idx doesn't exceed thresholds length\n",
    "best_idx = min(best_idx, len(thresholds) - 1)\n",
    "best_threshold = thresholds[best_idx] if len(thresholds) > 0 else 0.5\n",
    "print(f\"Chosen operating threshold (best F1 on val): {best_threshold:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7b3c65",
   "metadata": {},
   "source": [
    "## 10. Final Model Retraining\n",
    "\n",
    "Retrain the final model on combined training and validation data to maximize the use of available data for production deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34965f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain_on_full = True\n",
    "if retrain_on_full:\n",
    "    print(\"Retraining final model on train+val...\")\n",
    "    X_full = pd.concat([X_train_prep_df, X_val_prep_df], axis=0)\n",
    "    y_full = pd.concat([y_train, y_val], axis=0)\n",
    "    try:\n",
    "        final_model = fit_final_model(X_full.values, y_full.values, params=None, random_state=RANDOM_STATE)\n",
    "        print(\"Retrain finished.\")\n",
    "    except Exception:\n",
    "        try:\n",
    "            final_model.fit(X_full.values, y_full.values)\n",
    "            print(\"Retrain finished (sklearn fit).\")\n",
    "        except Exception as e:\n",
    "            print(\"Retrain failed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0b2fa3",
   "metadata": {},
   "source": [
    "## 11. Test Set Evaluation\n",
    "\n",
    "Evaluate the final model on the held-out test set to get an unbiased estimate of model performance. Calculate multiple metrics including:\n",
    "- ROC-AUC and PR-AUC\n",
    "- Precision, Recall, and F1 score at optimal threshold\n",
    "- Confusion matrix\n",
    "- Lift at top 20 percent\n",
    "\n",
    "Save the evaluation summary to a YAML file for documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddc7ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating final model on hold-out TEST set...\")\n",
    "y_test_proba = final_model.predict_proba(X_test_prep_df.values)[:,1]\n",
    "roc_test = roc_auc_score(y_test.values, y_test_proba)\n",
    "pr_test = average_precision_score(y_test.values, y_test_proba)\n",
    "print(f\"TEST ROC-AUC: {roc_test:.4f}\")\n",
    "print(f\"TEST PR-AUC:  {pr_test:.4f}\")\n",
    "\n",
    "# Compute threshold metrics on test using the pre-selected threshold\n",
    "test_metrics = compute_threshold_metrics(y_test.values, y_test_proba, threshold=best_threshold)\n",
    "print(\"Test metrics at chosen threshold:\")\n",
    "print(test_metrics)\n",
    "print(\"Confusion matrix:\")\n",
    "print(test_metrics['confusion_matrix'])\n",
    "\n",
    "# Compute Lift & Gain metrics (Top-K)\n",
    "top_k = config['EVALUATION'].get('LIFT_GAIN_TOP_K', 0.2)\n",
    "lift_results = compute_lift_at_k(y_test.values, y_test_proba, k=top_k)\n",
    "print(f\"Lift/Gain @ Top {top_k*100:.0f}%:\", lift_results)\n",
    "\n",
    "# Save evaluation summary (convert all numpy types to Python native types)\n",
    "eval_summary = {\n",
    "    'test': {\n",
    "        'roc_auc': float(roc_test),\n",
    "        'pr_auc': float(pr_test),\n",
    "        'threshold': float(best_threshold),\n",
    "        'precision': float(test_metrics['precision']),\n",
    "        'recall': float(test_metrics['recall']),\n",
    "        'f1': float(test_metrics['f1']),\n",
    "        'confusion_matrix': test_metrics['confusion_matrix'].tolist(),\n",
    "        'lift_at_top_k': {\n",
    "            'recall_at_k': float(lift_results['recall_at_k']),\n",
    "            'lift_at_k': float(lift_results['lift_at_k']) if not np.isnan(lift_results['lift_at_k']) else None\n",
    "        }\n",
    "    }\n",
    "}\n",
    "with open(os.path.join(OUTPUTS_DIR, 'evaluation_summary.yaml'), 'w') as f:\n",
    "    yaml.safe_dump(eval_summary, f)\n",
    "print(\"Saved evaluation summary to outputs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3622b4",
   "metadata": {},
   "source": [
    "## 12. Model Performance Visualizations\n",
    "\n",
    "Generate comprehensive visualizations to understand model behavior and performance across different metrics and perspectives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384d7177",
   "metadata": {},
   "source": [
    "### 12.1 Baseline Model Comparison\n",
    "\n",
    "Compare ROC-AUC and PR-AUC scores across all baseline models on the validation set. The best performing model is highlighted in green."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25f73a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# 1. Baseline models comparison\n",
    "print(\"Creating baseline models comparison chart...\")\n",
    "df_baseline = pd.DataFrame([\n",
    "    {'Model': name, 'ROC-AUC': metrics['roc_auc'], 'PR-AUC': metrics['pr_auc']}\n",
    "    for name, metrics in baseline_metrics.items()\n",
    "]).sort_values(by='ROC-AUC', ascending=False)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ROC-AUC comparison\n",
    "colors = ['#2ecc71' if model == best_name else '#95a5a6' for model in df_baseline['Model']]\n",
    "bars1 = ax1.bar(df_baseline['Model'], df_baseline['ROC-AUC'], color=colors)\n",
    "ax1.set_title('ROC-AUC Comparison (Validation Set)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Model', fontsize=11)\n",
    "ax1.set_ylabel('ROC-AUC Score', fontsize=11)\n",
    "ax1.set_ylim(df_baseline['ROC-AUC'].min() - 0.01, 1.0)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# PR-AUC comparison\n",
    "bars2 = ax2.bar(df_baseline['Model'], df_baseline['PR-AUC'], color=colors)\n",
    "ax2.set_title('PR-AUC Comparison (Validation Set)', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Model', fontsize=11)\n",
    "ax2.set_ylabel('PR-AUC Score', fontsize=11)\n",
    "ax2.set_ylim(df_baseline['PR-AUC'].min() - 0.01, 1.0)\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUTS_DIR, 'baseline_models_comparison.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {os.path.join(OUTPUTS_DIR, 'baseline_models_comparison.png')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b842a62",
   "metadata": {},
   "source": [
    "### 12.2 ROC Curve\n",
    "\n",
    "Receiver Operating Characteristic curve shows the trade-off between true positive rate and false positive rate across all classification thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574455a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. ROC Curve for final model on test set\n",
    "print(\"Creating ROC curve...\")\n",
    "fpr, tpr, _ = roc_curve(y_test.values, y_test_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (FPR)', fontsize=12)\n",
    "plt.ylabel('True Positive Rate (TPR)', fontsize=12)\n",
    "plt.title('ROC Curve - Test Set', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\", fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUTS_DIR, 'roc_curve_test.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {os.path.join(OUTPUTS_DIR, 'roc_curve_test.png')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551e5c9a",
   "metadata": {},
   "source": [
    "### 12.3 Precision-Recall Curve\n",
    "\n",
    "Precision-Recall curve is particularly useful for imbalanced datasets, showing the trade-off between precision and recall at different thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fab6c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Precision-Recall Curve\n",
    "print(\"Creating PR curve...\")\n",
    "precision_vals, recall_vals, pr_thresholds = precision_recall_curve(y_test.values, y_test_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall_vals, precision_vals, color='blue', lw=2, label=f'PR curve (AP = {pr_test:.4f})')\n",
    "plt.axhline(y=y_test.mean(), color='red', linestyle='--', lw=2, label=f'Baseline (No Skill = {y_test.mean():.4f})')\n",
    "plt.xlabel('Recall', fontsize=12)\n",
    "plt.ylabel('Precision', fontsize=12)\n",
    "plt.title('Precision-Recall Curve - Test Set', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"upper right\", fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUTS_DIR, 'precision_recall_curve_test.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {os.path.join(OUTPUTS_DIR, 'precision_recall_curve_test.png')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a662507",
   "metadata": {},
   "source": [
    "### 12.4 Confusion Matrix\n",
    "\n",
    "Visual representation of model predictions versus actual labels, showing true positives, true negatives, false positives, and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfa60fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Confusion Matrix Heatmap\n",
    "print(\"Creating confusion matrix heatmap...\")\n",
    "cm = test_metrics['confusion_matrix']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, \n",
    "            xticklabels=['No Churn', 'Churn'], \n",
    "            yticklabels=['No Churn', 'Churn'],\n",
    "            annot_kws={\"size\": 14})\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.title(f'Confusion Matrix - Test Set (Threshold = {best_threshold:.4f})', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUTS_DIR, 'confusion_matrix_test.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {os.path.join(OUTPUTS_DIR, 'confusion_matrix_test.png')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de5228f",
   "metadata": {},
   "source": [
    "### 12.5 Performance Metrics Summary\n",
    "\n",
    "Bar chart summarizing all key performance metrics on the test set for quick comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578c5246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Test Set Metrics Summary Bar Chart\n",
    "print(\"Creating test metrics summary chart...\")\n",
    "metrics_dict = {\n",
    "    'ROC-AUC': roc_test,\n",
    "    'PR-AUC': pr_test,\n",
    "    'Precision': test_metrics['precision'],\n",
    "    'Recall': test_metrics['recall'],\n",
    "    'F1-Score': test_metrics['f1']\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors_metrics = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6']\n",
    "bars = plt.bar(metrics_dict.keys(), metrics_dict.values(), color=colors_metrics, alpha=0.8, edgecolor='black')\n",
    "plt.ylim(0, 1.1)\n",
    "plt.ylabel('Score', fontsize=12)\n",
    "plt.title('Test Set Performance Metrics', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=15, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.4f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUTS_DIR, 'test_metrics_summary.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {os.path.join(OUTPUTS_DIR, 'test_metrics_summary.png')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ed348f",
   "metadata": {},
   "source": [
    "### 12.6 Cumulative Lift Chart\n",
    "\n",
    "Lift chart demonstrates how much better the model performs compared to random selection when targeting top-scored customers. This is particularly useful for campaign optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78c0b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Lift Chart\n",
    "print(\"Creating lift chart...\")\n",
    "# Sort by predicted probability descending\n",
    "df_lift = pd.DataFrame({\n",
    "    'y_true': y_test.values,\n",
    "    'y_proba': y_test_proba\n",
    "}).sort_values('y_proba', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Calculate cumulative statistics\n",
    "n_samples = len(df_lift)\n",
    "deciles = 10\n",
    "samples_per_decile = n_samples // deciles\n",
    "\n",
    "lift_data = []\n",
    "for i in range(1, deciles + 1):\n",
    "    top_n = samples_per_decile * i\n",
    "    top_samples = df_lift.iloc[:top_n]\n",
    "    \n",
    "    # Cumulative metrics\n",
    "    cum_actual_positive = top_samples['y_true'].sum()\n",
    "    cum_predicted_positive = top_n\n",
    "    baseline_positive = (df_lift['y_true'].sum() / n_samples) * top_n\n",
    "    \n",
    "    lift = cum_actual_positive / baseline_positive if baseline_positive > 0 else 0\n",
    "    \n",
    "    lift_data.append({\n",
    "        'Decile': i,\n",
    "        'Cumulative %': (top_n / n_samples) * 100,\n",
    "        'Lift': lift\n",
    "    })\n",
    "\n",
    "df_lift_plot = pd.DataFrame(lift_data)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_lift_plot['Cumulative %'], df_lift_plot['Lift'], \n",
    "         marker='o', linewidth=2, markersize=8, color='#e74c3c', label='Lift')\n",
    "plt.axhline(y=1, color='gray', linestyle='--', linewidth=2, label='Baseline (No Lift)')\n",
    "plt.xlabel('Cumulative % of Population (by predicted probability)', fontsize=12)\n",
    "plt.ylabel('Lift', fontsize=12)\n",
    "plt.title('Cumulative Lift Chart - Test Set', fontsize=14, fontweight='bold')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend(fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUTS_DIR, 'lift_chart_test.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {os.path.join(OUTPUTS_DIR, 'lift_chart_test.png')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9b69cd",
   "metadata": {},
   "source": [
    "### 12.7 Probability Distribution Analysis\n",
    "\n",
    "Analyze how predicted probabilities are distributed for churned and non-churned customers. Good separation indicates strong model discrimination ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06792818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Probability Distribution by Class\n",
    "print(\"Creating probability distribution plot...\")\n",
    "df_prob = pd.DataFrame({\n",
    "    'Probability': y_test_proba,\n",
    "    'Actual Class': ['Churn' if y == 1 else 'No Churn' for y in y_test.values]\n",
    "})\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(df_prob[df_prob['Actual Class'] == 'No Churn']['Probability'], \n",
    "             bins=50, alpha=0.7, label='No Churn', color='#3498db', edgecolor='black')\n",
    "axes[0].hist(df_prob[df_prob['Actual Class'] == 'Churn']['Probability'], \n",
    "             bins=50, alpha=0.7, label='Churn', color='#e74c3c', edgecolor='black')\n",
    "axes[0].axvline(x=best_threshold, color='green', linestyle='--', linewidth=2, label=f'Threshold = {best_threshold:.3f}')\n",
    "axes[0].set_xlabel('Predicted Probability', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Probability Distribution - Test Set', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "df_prob.boxplot(column='Probability', by='Actual Class', ax=axes[1], patch_artist=True)\n",
    "axes[1].set_xlabel('Actual Class', fontsize=12)\n",
    "axes[1].set_ylabel('Predicted Probability', fontsize=12)\n",
    "axes[1].set_title('Probability Distribution by Class', fontsize=13, fontweight='bold')\n",
    "axes[1].get_figure().suptitle('')  # Remove automatic title\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUTS_DIR, 'probability_distribution_test.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {os.path.join(OUTPUTS_DIR, 'probability_distribution_test.png')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a231eaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nVISUALIZATION SUMMARY\")\n",
    "print(f\"Created and saved 7 visualizations to directory: {OUTPUTS_DIR}\")\n",
    "print(\"\\nList of files:\")\n",
    "print(\"  1. baseline_models_comparison.png - ROC-AUC & PR-AUC comparison\")\n",
    "print(\"  2. roc_curve_test.png - ROC Curve\")\n",
    "print(\"  3. precision_recall_curve_test.png - PR Curve\")\n",
    "print(\"  4. confusion_matrix_test.png - Confusion Matrix\")\n",
    "print(\"  5. test_metrics_summary.png - Metrics summary\")\n",
    "print(\"  6. lift_chart_test.png - Lift Chart\")\n",
    "print(\"  7. probability_distribution_test.png - Probability distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515bcca1",
   "metadata": {},
   "source": [
    "## 13. Model Interpretability with SHAP\n",
    "\n",
    "Use SHAP (SHapley Additive exPlanations) to understand feature importance and how features contribute to model predictions. This provides transparency and helps build trust in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc79f3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running SHAP summary plot (may be slow)...\")\n",
    "try:\n",
    "    explainer = shap.TreeExplainer(final_model)\n",
    "    # Use validation set for SHAP (smaller, representative sample)\n",
    "    shap_sample = X_val_prep_df.sample(min(500, len(X_val_prep_df)), random_state=RANDOM_STATE)\n",
    "    shap_values = explainer.shap_values(shap_sample.values)\n",
    "    \n",
    "    # Handle different SHAP output formats for binary classification\n",
    "    # LightGBM/XGBoost may return shape (n_samples, n_features) or (2, n_samples, n_features)\n",
    "    if isinstance(shap_values, list):\n",
    "        # If list, take positive class (index 1)\n",
    "        shap_values_plot = shap_values[1]\n",
    "    elif len(shap_values.shape) == 3:\n",
    "        # If 3D array, take positive class (index 1)\n",
    "        shap_values_plot = shap_values[1]\n",
    "    else:\n",
    "        # If 2D array, use as-is\n",
    "        shap_values_plot = shap_values\n",
    "    \n",
    "    # Create SHAP summary plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values_plot, shap_sample, show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUTS_DIR, 'shap_summary.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Saved SHAP summary plot.\")\n",
    "except Exception as e:\n",
    "    print(\"SHAP summary failed or not supported for this model:\", e)\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365c6c8e",
   "metadata": {},
   "source": [
    "## 14. Save Model Artifacts\n",
    "\n",
    "Save the trained model, preprocessor, and feature names for production deployment. These artifacts can be loaded later for making predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77022124",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(OUTPUTS_DIR, 'final_model.pkl')\n",
    "preproc_path = os.path.join(OUTPUTS_DIR, 'preprocessor.pkl')\n",
    "joblib.dump(final_model, model_path)\n",
    "joblib.dump(preprocessor, preproc_path)\n",
    "print(\"Saved final model to:\", model_path)\n",
    "print(\"Saved preprocessor to:\", preproc_path)\n",
    "\n",
    "# Save selected features list\n",
    "with open(os.path.join(OUTPUTS_DIR, 'selected_features.txt'), 'w') as f:\n",
    "    for fn in feature_names:\n",
    "        f.write(fn + \"\\n\")\n",
    "print(\"Saved selected feature names.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096194bb",
   "metadata": {},
   "source": [
    "## 15. Final Report Summary\n",
    "\n",
    "Consolidated summary of all key results including model performance metrics, optimal threshold, and artifact locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77022124",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FINAL REPORT\")\n",
    "print(\"Model path:\", model_path)\n",
    "print(\"ROC-AUC (test):\", roc_test)\n",
    "print(\"PR-AUC (test):\", pr_test)\n",
    "print(\"Threshold:\", best_threshold)\n",
    "print(\"Precision/Recall/F1 (test):\", test_metrics['precision'], test_metrics['recall'], test_metrics['f1'])\n",
    "print(\"Lift@Top20%:\", lift_results)\n",
    "print(\"Artifacts saved to:\", OUTPUTS_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
