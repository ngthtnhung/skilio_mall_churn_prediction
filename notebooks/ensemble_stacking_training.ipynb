{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12c7fd7d",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55a0cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score,\n",
    "    precision_recall_curve, precision_score,\n",
    "    recall_score, f1_score, confusion_matrix,\n",
    "    roc_curve, auc\n",
    ")\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Set up project paths\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.insert(0, os.path.join(project_root, 'src'))\n",
    "\n",
    "# Load configuration\n",
    "config_path = os.path.join(project_root, 'config.yaml')\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "RANDOM_STATE = config['GENERAL']['RANDOM_STATE']\n",
    "TARGET_COL = config['GENERAL']['TARGET_COLUMN']\n",
    "OUTPUTS_DIR = os.path.join(project_root, 'outputs', 'ensemble')\n",
    "os.makedirs(OUTPUTS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Random state: {RANDOM_STATE}\")\n",
    "print(f\"Target column: {TARGET_COL}\")\n",
    "print(f\"Outputs directory: outputs/ensemble/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee3ce4b",
   "metadata": {},
   "source": [
    "## 2. Load Preprocessed Data from Checkpoint\n",
    "\n",
    "Load the train/val/test splits and preprocessed features that were created in the main training notebook. This ensures consistency and saves processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ca77de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint data (already split and feature-engineered)\n",
    "checkpoint_path = os.path.join(project_root, config['PATHS']['CHECKPOINT_DATA'])\n",
    "\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Checkpoint file not found: {checkpoint_path}\\n\"\n",
    "        \"Please run 'churn_model_training.ipynb' first to generate the checkpoint.\"\n",
    "    )\n",
    "\n",
    "data_splits = pd.read_pickle(checkpoint_path)\n",
    "train_df = data_splits['train']\n",
    "val_df = data_splits['val']\n",
    "test_df = data_splits['test']\n",
    "\n",
    "print(\"Loaded data splits:\")\n",
    "print(f\"  Train: {train_df.shape}\")\n",
    "print(f\"  Val:   {val_df.shape}\")\n",
    "print(f\"  Test:  {test_df.shape}\")\n",
    "print(f\"\\nChurn distribution (train): {train_df[TARGET_COL].value_counts(normalize=True).to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cfd711",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering and Preprocessing\n",
    "\n",
    "Apply the same feature engineering and preprocessing pipeline used in the baseline training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6240b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom modules\n",
    "try:\n",
    "    from features import create_all_features\n",
    "    from preprocessing import create_preprocessor\n",
    "    print(\"Successfully imported custom modules\")\n",
    "except ImportError as e:\n",
    "    print(f\"Warning: Could not import custom modules: {e}\")\n",
    "    print(\"Using fallback implementations...\")\n",
    "    \n",
    "    def create_all_features(df):\n",
    "        return df.copy()\n",
    "    \n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    \n",
    "    def create_preprocessor(num_feats, cat_feats):\n",
    "        num_pipe = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        cat_pipe = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "        ])\n",
    "        return ColumnTransformer([\n",
    "            ('num', num_pipe, num_feats),\n",
    "            ('cat', cat_pipe, cat_feats),\n",
    "        ], sparse_threshold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584c3c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature engineering\n",
    "print(\"Applying feature engineering...\")\n",
    "train_fe = create_all_features(train_df)\n",
    "val_fe = create_all_features(val_df)\n",
    "test_fe = create_all_features(test_df)\n",
    "\n",
    "# Separate features and target\n",
    "X_train = train_fe.drop(columns=[TARGET_COL])\n",
    "y_train = train_fe[TARGET_COL].astype(int)\n",
    "\n",
    "X_val = val_fe.drop(columns=[TARGET_COL])\n",
    "y_val = val_fe[TARGET_COL].astype(int)\n",
    "\n",
    "X_test = test_fe.drop(columns=[TARGET_COL])\n",
    "y_test = test_fe[TARGET_COL].astype(int)\n",
    "\n",
    "print(f\"After feature engineering: {X_train.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a38568c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical and categorical features\n",
    "num_feats = config['FEATURE_ENGINEERING']['NUMERICAL_FEATURES']\n",
    "cat_feats = config['FEATURE_ENGINEERING']['CATEGORICAL_FEATURES']\n",
    "\n",
    "print(f\"Feature breakdown: {len(num_feats)} numerical, {len(cat_feats)} categorical\")\n",
    "\n",
    "# Create and fit preprocessor\n",
    "print(\"\\nFitting preprocessor on training data only...\")\n",
    "preprocessor = create_preprocessor(num_feats, cat_feats)\n",
    "X_train_prep = preprocessor.fit_transform(X_train)\n",
    "X_val_prep = preprocessor.transform(X_val)\n",
    "X_test_prep = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"Preprocessed shape: {X_train_prep.shape}\")\n",
    "print(\"Preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb80f1a",
   "metadata": {},
   "source": [
    "## 4. Define Base Models for Stacking\n",
    "\n",
    "Configure diverse base models with parameters optimized for churn prediction. Each model brings different strengths:\n",
    "- **Random Forest:** Robust to outliers, handles non-linearity well\n",
    "- **XGBoost:** Strong performance on structured data, handles imbalance\n",
    "- **LightGBM:** Fast training, efficient memory usage\n",
    "- **Logistic Regression:** Linear baseline, interpretable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d66fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base models (Level 0)\n",
    "print(\"Configuring base models for stacking...\\n\")\n",
    "\n",
    "base_models = [\n",
    "    ('rf', RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=12,\n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=5,\n",
    "        class_weight='balanced',\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1\n",
    "    )),\n",
    "    \n",
    "    ('xgb', xgb.XGBClassifier(\n",
    "        n_estimators=500,\n",
    "        max_depth=7,\n",
    "        learning_rate=0.05,\n",
    "        scale_pos_weight=3,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        eval_metric='logloss'\n",
    "    )),\n",
    "    \n",
    "    ('lgbm', lgb.LGBMClassifier(\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.03,\n",
    "        max_depth=7,\n",
    "        class_weight='balanced',\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )),\n",
    "    \n",
    "    ('lr', LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        solver='lbfgs',\n",
    "        class_weight='balanced',\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "]\n",
    "\n",
    "print(\"Base models configured:\")\n",
    "for name, model in base_models:\n",
    "    print(f\"  - {name.upper()}: {model.__class__.__name__}\")\n",
    "\n",
    "# Meta-learner (Level 1)\n",
    "meta_learner = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f\"\\nMeta-learner: {meta_learner.__class__.__name__}\")\n",
    "print(\"  → Will learn optimal combination of base model predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc2c1c3",
   "metadata": {},
   "source": [
    "## 5. Train Individual Base Models\n",
    "\n",
    "First, train each base model individually to evaluate their standalone performance before stacking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3adca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training individual base models...\\n\")\n",
    "\n",
    "base_results = {}\n",
    "\n",
    "for name, model in base_models:\n",
    "    print(f\"Training {name.upper()}...\")\n",
    "    \n",
    "    # Train on training set\n",
    "    model.fit(X_train_prep, y_train)\n",
    "    \n",
    "    # Predict on validation set\n",
    "    val_proba = model.predict_proba(X_val_prep)[:, 1]\n",
    "    val_roc = roc_auc_score(y_val, val_proba)\n",
    "    val_pr = average_precision_score(y_val, val_proba)\n",
    "    \n",
    "    base_results[name] = {\n",
    "        'model': model,\n",
    "        'val_roc_auc': val_roc,\n",
    "        'val_pr_auc': val_pr,\n",
    "        'val_proba': val_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"ROC-AUC: {val_roc:.4f} | PR-AUC: {val_pr:.4f}\\n\")\n",
    "\n",
    "print(\"\\nBase Model Performance Summary (Validation Set):\")\n",
    "for name, results in base_results.items():\n",
    "    print(f\"{name.upper():6s} - ROC-AUC: {results['val_roc_auc']:.4f} | PR-AUC: {results['val_pr_auc']:.4f}\")\n",
    "\n",
    "best_base = max(base_results.items(), key=lambda x: x[1]['val_roc_auc'])\n",
    "print(f\"\\nBest individual model: {best_base[0].upper()} ({best_base[1]['val_roc_auc']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685ffb2a",
   "metadata": {},
   "source": [
    "## 6. Build and Train Stacking Ensemble\n",
    "\n",
    "Create a StackingClassifier that combines predictions from all base models using a meta-learner. The stacking approach uses cross-validation to generate out-of-fold predictions, preventing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d54781d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building Stacking Ensemble...\\n\")\n",
    "\n",
    "# Create stacking classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_learner,\n",
    "    cv=5,  # 5-fold cross-validation for meta-features\n",
    "    stack_method='predict_proba',  # Use probabilities instead of class predictions\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"Stacking Classifier Configuration:\")\n",
    "print(f\"  Base Models: {len(base_models)}\")\n",
    "print(f\"  Meta-Learner: {meta_learner.__class__.__name__}\")\n",
    "print(f\"  CV Folds: 5\")\n",
    "print(f\"  Stack Method: predict_proba\\n\")\n",
    "\n",
    "print(\"Training stacking ensemble (this may take a few minutes)...\")\n",
    "stacking_model.fit(X_train_prep, y_train)\n",
    "print(\"Stacking ensemble training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1da769a",
   "metadata": {},
   "source": [
    "## 7. Evaluate Stacking Ensemble on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d665aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating stacking ensemble on validation set...\\n\")\n",
    "\n",
    "# Predict on validation set\n",
    "val_proba_stacking = stacking_model.predict_proba(X_val_prep)[:, 1]\n",
    "val_roc_stacking = roc_auc_score(y_val, val_proba_stacking)\n",
    "val_pr_stacking = average_precision_score(y_val, val_proba_stacking)\n",
    "\n",
    "print(\"VALIDATION SET PERFORMANCE COMPARISON\")\n",
    "\n",
    "# Show all base models\n",
    "print(\"\\nIndividual Base Models:\")\n",
    "for name, results in sorted(base_results.items(), key=lambda x: x[1]['val_roc_auc'], reverse=True):\n",
    "    print(f\"  {name.upper():6s} - ROC-AUC: {results['val_roc_auc']:.4f} | PR-AUC: {results['val_pr_auc']:.4f}\")\n",
    "\n",
    "# Show stacking performance\n",
    "print(f\"\\nStacking Ensemble:\")\n",
    "print(f\"  STACK  - ROC-AUC: {val_roc_stacking:.4f} | PR-AUC: {val_pr_stacking:.4f}\")\n",
    "\n",
    "# Calculate improvement\n",
    "improvement_roc = val_roc_stacking - best_base[1]['val_roc_auc']\n",
    "improvement_pr = val_pr_stacking - best_base[1]['val_pr_auc']\n",
    "\n",
    "print(\"IMPROVEMENT OVER BEST BASE MODEL:\")\n",
    "print(f\"  ROC-AUC: {improvement_roc:+.4f} ({improvement_roc*100:+.2f}%)\")\n",
    "print(f\"  PR-AUC:  {improvement_pr:+.4f} ({improvement_pr*100:+.2f}%)\")\n",
    "\n",
    "if improvement_roc > 0:\n",
    "    print(\"\\nStacking ensemble IMPROVES over best individual model!\")\n",
    "else:\n",
    "    print(\"\\nStacking ensemble does NOT improve over best individual model\")\n",
    "    print(\"    → Best base model may already be optimal for this dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094f1457",
   "metadata": {},
   "source": [
    "## 8. Optimal Threshold Selection\n",
    "\n",
    "Find the optimal classification threshold by maximizing F1 score on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6587c126",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Selecting optimal threshold on validation set...\\n\")\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_val, val_proba_stacking)\n",
    "f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-12)\n",
    "best_idx = np.nanargmax(f1_scores)\n",
    "best_idx = min(best_idx, len(thresholds) - 1)\n",
    "best_threshold = thresholds[best_idx] if len(thresholds) > 0 else 0.5\n",
    "\n",
    "print(f\"Optimal threshold (max F1 on validation): {best_threshold:.4f}\")\n",
    "print(f\"  → Precision: {precisions[best_idx]:.4f}\")\n",
    "print(f\"  → Recall:    {recalls[best_idx]:.4f}\")\n",
    "print(f\"  → F1-Score:  {f1_scores[best_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8c7334",
   "metadata": {},
   "source": [
    "## 9. Retrain on Combined Train+Val for Final Model\n",
    "\n",
    "Retrain the stacking ensemble on the combined training and validation sets to maximize available data for production deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da08051",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Retraining stacking ensemble on train+val combined...\\n\")\n",
    "\n",
    "X_full = np.vstack([X_train_prep, X_val_prep])\n",
    "y_full = pd.concat([y_train, y_val], axis=0)\n",
    "\n",
    "print(f\"Combined training data: {X_full.shape}\")\n",
    "\n",
    "# Rebuild stacking model with same configuration\n",
    "final_stacking_model = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_learner,\n",
    "    cv=5,\n",
    "    stack_method='predict_proba',\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"Training final stacking model...\")\n",
    "final_stacking_model.fit(X_full, y_full)\n",
    "print(\"Final model training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626bdb49",
   "metadata": {},
   "source": [
    "## 10. Final Evaluation on Test Set\n",
    "\n",
    "Evaluate the final stacking ensemble on the held-out test set to get an unbiased estimate of production performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384ca14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating final stacking model on TEST set...\\n\")\n",
    "\n",
    "# Predict on test set\n",
    "y_test_proba = final_stacking_model.predict_proba(X_test_prep)[:, 1]\n",
    "y_test_pred = (y_test_proba >= best_threshold).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "test_roc = roc_auc_score(y_test, y_test_proba)\n",
    "test_pr = average_precision_score(y_test, y_test_proba)\n",
    "test_precision = precision_score(y_test, y_test_pred, zero_division=0)\n",
    "test_recall = recall_score(y_test, y_test_pred, zero_division=0)\n",
    "test_f1 = f1_score(y_test, y_test_pred, zero_division=0)\n",
    "test_cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Calculate lift at top 20%\n",
    "df_lift = pd.DataFrame({'y': y_test.values, 'p': y_test_proba}).sort_values('p', ascending=False)\n",
    "top_20_idx = int(len(df_lift) * 0.2)\n",
    "lift_recall = df_lift.iloc[:top_20_idx]['y'].sum() / df_lift['y'].sum()\n",
    "lift_value = (df_lift.iloc[:top_20_idx]['y'].mean() / df_lift['y'].mean())\n",
    "\n",
    "print(\"FINAL TEST SET PERFORMANCE - STACKING ENSEMBLE\")\n",
    "print(f\"ROC-AUC:       {test_roc:.4f}\")\n",
    "print(f\"PR-AUC:        {test_pr:.4f}\")\n",
    "print(f\"Precision:     {test_precision:.4f}\")\n",
    "print(f\"Recall:        {test_recall:.4f}\")\n",
    "print(f\"F1-Score:      {test_f1:.4f}\")\n",
    "print(f\"Threshold:     {best_threshold:.4f}\")\n",
    "print(f\"\\nLift @ Top 20%: {lift_value:.3f}x\")\n",
    "print(f\"Recall @ Top 20%: {lift_recall:.2%}\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(test_cm)\n",
    "print(f\"  TN: {test_cm[0,0]:,} | FP: {test_cm[0,1]:,}\")\n",
    "print(f\"  FN: {test_cm[1,0]:,} | TP: {test_cm[1,1]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71328ac9",
   "metadata": {},
   "source": [
    "## 11. Compare with Baseline Single Model\n",
    "\n",
    "Load the baseline model results and compare with stacking ensemble performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcd66e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline results if available\n",
    "baseline_eval_path = os.path.join(project_root, 'outputs', 'evaluation_summary.yaml')\n",
    "\n",
    "if os.path.exists(baseline_eval_path):\n",
    "    with open(baseline_eval_path, 'r') as f:\n",
    "        baseline_results = yaml.safe_load(f)\n",
    "    \n",
    "    baseline_test = baseline_results['test']\n",
    "    \n",
    "    print(\"COMPARISON: STACKING vs BASELINE SINGLE MODEL\")\n",
    "    print(f\"\\n{'Metric':<20s} {'Baseline':<12s} {'Stacking':<12s} {'Improvement':<12s}\")\n",
    "    \n",
    "    metrics_comparison = [\n",
    "        ('ROC-AUC', baseline_test['roc_auc'], test_roc),\n",
    "        ('PR-AUC', baseline_test['pr_auc'], test_pr),\n",
    "        ('F1-Score', baseline_test['f1'], test_f1),\n",
    "        ('Precision', baseline_test['precision'], test_precision),\n",
    "        ('Recall', baseline_test['recall'], test_recall),\n",
    "    ]\n",
    "    \n",
    "    for metric_name, baseline_val, stacking_val in metrics_comparison:\n",
    "        improvement = stacking_val - baseline_val\n",
    "        improvement_str = f\"{improvement:+.4f}\"\n",
    "        print(f\"{metric_name:<20s} {baseline_val:<12.4f} {stacking_val:<12.4f} {improvement_str:<12s}\")\n",
    "    \n",
    "    \n",
    "    if test_roc > baseline_test['roc_auc']:\n",
    "        print(\"RESULT: Stacking ensemble OUTPERFORMS baseline model!\")\n",
    "        print(f\"   → Improvement: {(test_roc - baseline_test['roc_auc'])*100:.2f}% ROC-AUC\")\n",
    "    elif test_roc < baseline_test['roc_auc']:\n",
    "        print(\"RESULT: Baseline model performs better than stacking\")\n",
    "        print(f\"   → The simple Logistic Regression was already near-optimal\")\n",
    "    else:\n",
    "        print(\"RESULT: Both models perform equally well\")\n",
    "    \n",
    "else:\n",
    "    print(\"Baseline results not found. Run 'churn_model_training.ipynb' first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96defbf2",
   "metadata": {},
   "source": [
    "## 12. Visualization: ROC Curves Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7215eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for all models\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Plot base models\n",
    "for name, results in base_results.items():\n",
    "    test_proba_base = results['model'].predict_proba(X_test_prep)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, test_proba_base)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=1.5, alpha=0.7, label=f'{name.upper()} (AUC = {roc_auc:.4f})')\n",
    "\n",
    "# Plot stacking ensemble\n",
    "fpr_stack, tpr_stack, _ = roc_curve(y_test, y_test_proba)\n",
    "roc_auc_stack = auc(fpr_stack, tpr_stack)\n",
    "plt.plot(fpr_stack, tpr_stack, lw=3, color='red', label=f'STACKING (AUC = {roc_auc_stack:.4f})')\n",
    "\n",
    "# Plot random classifier\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curves: Stacking vs Base Models (Test Set)', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUTS_DIR, 'roc_curves_comparison.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Saved: {os.path.join(OUTPUTS_DIR, 'roc_curves_comparison.png')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e4cca8",
   "metadata": {},
   "source": [
    "## 13. Visualization: Model Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c6df7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance comparison bar chart\n",
    "models_names = [name.upper() for name in base_results.keys()] + ['STACKING']\n",
    "\n",
    "# Get test ROC-AUC for each base model\n",
    "test_roc_scores = []\n",
    "for name, results in base_results.items():\n",
    "    test_proba_base = results['model'].predict_proba(X_test_prep)[:, 1]\n",
    "    test_roc_scores.append(roc_auc_score(y_test, test_proba_base))\n",
    "test_roc_scores.append(test_roc)\n",
    "\n",
    "# Create bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "colors = ['#3498db', '#2ecc71', '#f39c12', '#9b59b6', '#e74c3c']\n",
    "bars = ax.bar(models_names, test_roc_scores, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# Highlight the best model\n",
    "best_idx = np.argmax(test_roc_scores)\n",
    "bars[best_idx].set_edgecolor('darkred')\n",
    "bars[best_idx].set_linewidth(3)\n",
    "\n",
    "ax.set_ylabel('ROC-AUC Score', fontsize=12)\n",
    "ax.set_title('Model Performance Comparison - Test Set', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(min(test_roc_scores) - 0.01, 1.0)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, score in zip(bars, test_roc_scores):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{score:.4f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUTS_DIR, 'models_performance_comparison.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Saved: {os.path.join(OUTPUTS_DIR, 'models_performance_comparison.png')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2888ee",
   "metadata": {},
   "source": [
    "## 14. Save Stacking Model and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168753ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final stacking model\n",
    "model_path = os.path.join(OUTPUTS_DIR, 'stacking_model.pkl')\n",
    "joblib.dump(final_stacking_model, model_path)\n",
    "print(\"Saved stacking model: outputs/ensemble/stacking_model.pkl\")\n",
    "\n",
    "# Save evaluation results\n",
    "ensemble_results = {\n",
    "    'stacking_ensemble': {\n",
    "        'test': {\n",
    "            'roc_auc': float(test_roc),\n",
    "            'pr_auc': float(test_pr),\n",
    "            'f1': float(test_f1),\n",
    "            'precision': float(test_precision),\n",
    "            'recall': float(test_recall),\n",
    "            'threshold': float(best_threshold),\n",
    "            'confusion_matrix': test_cm.tolist(),\n",
    "            'lift_at_top_20': {\n",
    "                'lift': float(lift_value),\n",
    "                'recall': float(lift_recall)\n",
    "            }\n",
    "        },\n",
    "        'base_models': {\n",
    "            name: {\n",
    "                'val_roc_auc': float(results['val_roc_auc']),\n",
    "                'val_pr_auc': float(results['val_pr_auc'])\n",
    "            }\n",
    "            for name, results in base_results.items()\n",
    "        },\n",
    "        'config': {\n",
    "            'base_models_count': len(base_models),\n",
    "            'meta_learner': meta_learner.__class__.__name__,\n",
    "            'cv_folds': 5,\n",
    "            'random_state': RANDOM_STATE\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "results_path = os.path.join(OUTPUTS_DIR, 'stacking_evaluation_summary.yaml')\n",
    "with open(results_path, 'w') as f:\n",
    "    yaml.safe_dump(ensemble_results, f)\n",
    "print(\"Saved evaluation results: outputs/ensemble/stacking_evaluation_summary.yaml\")\n",
    "\n",
    "print(\"\\nAll artifacts saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7af3b2",
   "metadata": {},
   "source": [
    "## 15. Final Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868b68e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STACKING ENSEMBLE TRAINING - FINAL SUMMARY\\n\")\n",
    "\n",
    "print(\"MODEL ARCHITECTURE:\")\n",
    "print(f\"  Base Models (Level 0): {len(base_models)}\")\n",
    "for name, _ in base_models:\n",
    "    print(f\"    - {name.upper()}\")\n",
    "print(f\"  Meta-Learner (Level 1): {meta_learner.__class__.__name__}\")\n",
    "print(f\"  Cross-Validation: 5-fold\")\n",
    "\n",
    "print(\"\\nTEST SET PERFORMANCE:\")\n",
    "print(f\"  ROC-AUC:   {test_roc:.4f}\")\n",
    "print(f\"  PR-AUC:    {test_pr:.4f}\")\n",
    "print(f\"  F1-Score:  {test_f1:.4f}\")\n",
    "print(f\"  Precision: {test_precision:.4f}\")\n",
    "print(f\"  Recall:    {test_recall:.4f}\")\n",
    "\n",
    "print(\"\\nBUSINESS IMPACT:\")\n",
    "print(f\"  Lift @ Top 20%: {lift_value:.2f}x\")\n",
    "print(f\"  Targeting top 20% captures {lift_recall:.1%} of churners\")\n",
    "print(f\"  {lift_value:.2f}x more effective than random targeting\")\n",
    "\n",
    "print(\"\\nSAVED ARTIFACTS:\")\n",
    "print(\"  - Model:         outputs/ensemble/stacking_model.pkl\")\n",
    "print(\"  - Results:       outputs/ensemble/stacking_evaluation_summary.yaml\")\n",
    "print(\"  - Visualizations: outputs/ensemble/*.png (2 charts)\")\n",
    "\n",
    "if os.path.exists(baseline_eval_path):\n",
    "    improvement = test_roc - baseline_test['roc_auc']\n",
    "    if improvement > 0.001:\n",
    "        print(\"\\nRECOMMENDATION: Use stacking ensemble for production\")\n",
    "        print(f\"  Improvement: +{improvement*100:.2f}% ROC-AUC over baseline\")\n",
    "    else:\n",
    "        print(\"\\nRECOMMENDATION: Consider using baseline model for production\")\n",
    "        print(\"  Simpler, faster inference with comparable performance\")\n",
    "        print(\"  Stacking adds complexity without significant gain\")\n",
    "else:\n",
    "    print(\"\\nRun baseline training notebook to enable comparison\")\n",
    "\n",
    "print(\"\\nStacking ensemble training complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
